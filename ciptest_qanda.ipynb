{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7eef9-23a4-430a-bc89-d3002293e276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import secrets\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pdb\n",
    "from typing import Any\n",
    "\n",
    "from etl import markdown, pdfs, shared, videos\n",
    "\n",
    "import docstore\n",
    "import vecstore\n",
    "from utils import pretty_log\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import textwrap\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import langchain\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "import ciptest_qanda as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b544796-7749-4ab5-888d-476eb2feb652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import textwrap\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "embedding_engine = vecstore.get_embedding_engine(allowed_special=\"all\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                          token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             token=True,\n",
    "                                             #  load_in_8bit=True,\n",
    "                                             #  load_in_4bit=True,\n",
    "                                             )\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens=4096,\n",
    "                do_sample=True,\n",
    "                # temperature=0.1,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                streamer=streamer,\n",
    "                )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "pretty_log(\"connecting to vector storage\")\n",
    "vector_index = vecstore.connect_to_vector_index(vecstore.INDEX_NAME, embedding_engine)\n",
    "pretty_log(\"connected to vector storage\")\n",
    "pretty_log(f\"found {vector_index.index.ntotal} vectors to search over\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45dd9d8-6f8f-4805-9935-664c46317b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## THE LAMA 2 DEMO ##############\n",
    "\n",
    "template = \"\"\"\n",
    "[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n",
    "{context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "retriever = vector_index.as_retriever(\n",
    "        search_kwargs={\"k\": 6}\n",
    "    )\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,     \n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e5a85-5a0e-48dc-a0f1-2f0f777080fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"What models use human instructions?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591e0b55-dedb-4553-b286-3dd1ac943eeb",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-01-11T15:54:44.754946900Z",
     "start_time": "2024-01-11T15:54:44.749947200Z"
    }
   },
   "outputs": [],
   "source": [
    "########## THE LAMA 2 DEMO ############## - MORE GENERIC AND CUSTOMIZED \n",
    "langchain.debug=False \n",
    "\n",
    "llama_docs_template = \"\"\"\n",
    "[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n",
    "{context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "llama_docs_prompt = PromptTemplate(template=llama_docs_template, input_variables=[\"context\", \"question\"])\n",
    "llama_doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", prompt= llama_docs_prompt, document_variable_name=\"context\", verbose=False)\n",
    "\n",
    "llama_condense_template = \"\"\"\n",
    "[INST]Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question: [/INST]\"\"\"\n",
    "llama_condense_prompt = PromptTemplate(template=llama_condense_template, input_variables=[\"chat_history\", \"question\"])\n",
    "llama_question_generator_chain = LLMChain(llm=llm, prompt=llama_condense_prompt, verbose=False)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llama_v2_chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_index.as_retriever(search_kwargs={'k': 6}),\n",
    "    question_generator=llama_question_generator_chain,\n",
    "    combine_docs_chain=llama_doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae01051-b5ea-47b2-a521-291ed6771156",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-01-11T15:55:29.123768700Z",
     "start_time": "2024-01-11T15:54:49.735822400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speaker discusses the use of large language models in AI and their potential to revolutionize safety and security. They mention the importance of using these models in a way that is safe and secure, and how they can be trained to learn from human instructions. The speaker also discusses the potential for these models to be used in various domains, such as health, safety, and cybersecurity, and how they can be used to make people safer.\n",
      "In terms of the question about models using human instructions, the speaker mentions that reinforcement learning with human feedback can be used to train these models. They explain that the user can provide feedback to the model, which can then improve its performance, making it more accurate and effective. The speaker also notes that this approach can democratize the use of AI, as people can contribute their expertise and knowledge to the model's training.\n",
      "Overall, the speaker emphasizes the potential of large language models in AI to revolutionize various fields and make a significant impact on society. They also highlight the importance of ensuring that these models are used in a responsible and ethical manner, with a focus on safety and security.</s>\n",
      "{'question': 'What models use human instructions?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content=\"The speaker discusses the use of large language models in AI and their potential to revolutionize safety and security. They mention the importance of using these models in a way that is safe and secure, and how they can be trained to learn from human instructions. The speaker also discusses the potential for these models to be used in various domains, such as health, safety, and cybersecurity, and how they can be used to make people safer.\\n\\nIn terms of the question about models using human instructions, the speaker mentions that reinforcement learning with human feedback can be used to train these models. They explain that the user can provide feedback to the model, which can then improve its performance, making it more accurate and effective. The speaker also notes that this approach can democratize the use of AI, as people can contribute their expertise and knowledge to the model's training.\\n\\nOverall, the speaker emphasizes the potential of large language models in AI to revolutionize various fields and make a significant impact on society. They also highlight the importance of ensuring that these models are used in a responsible and ethical manner, with a focus on safety and security.\")], 'answer': \"The speaker discusses the use of large language models in AI and their potential to revolutionize safety and security. They mention the importance of using these models in a way that is safe and secure, and how they can be trained to learn from human instructions. The speaker also discusses the potential for these models to be used in various domains, such as health, safety, and cybersecurity, and how they can be used to make people safer.\\n\\nIn terms of the question about models using human instructions, the speaker mentions that reinforcement learning with human feedback can be used to train these models. They explain that the user can provide feedback to the model, which can then improve its performance, making it more accurate and effective. The speaker also notes that this approach can democratize the use of AI, as people can contribute their expertise and knowledge to the model's training.\\n\\nOverall, the speaker emphasizes the potential of large language models in AI to revolutionize various fields and make a significant impact on society. They also highlight the importance of ensuring that these models are used in a responsible and ethical manner, with a focus on safety and security.\"}\n",
      " Standalone question: What are the advantages of using reinforcement learning with human feedback to train large language models in AI?</s>\n",
      "The speaker is discussing the potential advantages of using reinforcement learning with human feedback to train large language models in AI. The speaker notes that this approach can provide several benefits, including:\n",
      "1. Exponential improvement: The speaker suggests that the use of reinforcement learning with human feedback can lead to an exponential improvement in the performance of large language models.\n",
      "2. Democratization of AI: The speaker believes that the use of reinforcement learning with human feedback can democratize AI by allowing non-experts to contribute to the training process.\n",
      "3. Improved accuracy: The speaker notes that the use of human feedback can lead to more accurate predictions and better decision-making in AI systems.\n",
      "4. Increased efficiency: The speaker suggests that the use of reinforcement learning with human feedback can lead to more efficient training of large language models, as the feedback can help guide the training process.\n",
      "5. Better handling of complex tasks: The speaker believes that reinforcement learning with human feedback can be particularly effective in handling complex tasks that require a high degree of nuance and understanding.\n",
      "6. Improved interpretability: The speaker notes that the use of reinforcement learning with human feedback can lead to more interpretable AI systems, as the feedback can help provide insights into how the systems are making decisions.\n",
      "7. Enhanced safety and security: The speaker suggests that the use of reinforcement learning with human feedback can enhance safety and security in AI systems by allowing for more careful and deliberate decision-making.\n",
      "8. Increased trust in AI: The speaker believes that the use of reinforcement learning with human feedback can increase trust in AI systems by providing a more transparent and accountable training process.\n",
      "Overall, the speaker seems to be highlighting the potential advantages of using reinforcement learning with human feedback to train large language models in AI, particularly in terms of improved accuracy, efficiency, and safety.</s>\n",
      "{'question': 'Which are the advantage of each of these models?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content=\"The speaker discusses the use of large language models in AI and their potential to revolutionize safety and security. They mention the importance of using these models in a way that is safe and secure, and how they can be trained to learn from human instructions. The speaker also discusses the potential for these models to be used in various domains, such as health, safety, and cybersecurity, and how they can be used to make people safer.\\n\\nIn terms of the question about models using human instructions, the speaker mentions that reinforcement learning with human feedback can be used to train these models. They explain that the user can provide feedback to the model, which can then improve its performance, making it more accurate and effective. The speaker also notes that this approach can democratize the use of AI, as people can contribute their expertise and knowledge to the model's training.\\n\\nOverall, the speaker emphasizes the potential of large language models in AI to revolutionize various fields and make a significant impact on society. They also highlight the importance of ensuring that these models are used in a responsible and ethical manner, with a focus on safety and security.\"), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content='\\nThe speaker is discussing the potential advantages of using reinforcement learning with human feedback to train large language models in AI. The speaker notes that this approach can provide several benefits, including:\\n\\n1. Exponential improvement: The speaker suggests that the use of reinforcement learning with human feedback can lead to an exponential improvement in the performance of large language models.\\n2. Democratization of AI: The speaker believes that the use of reinforcement learning with human feedback can democratize AI by allowing non-experts to contribute to the training process.\\n3. Improved accuracy: The speaker notes that the use of human feedback can lead to more accurate predictions and better decision-making in AI systems.\\n4. Increased efficiency: The speaker suggests that the use of reinforcement learning with human feedback can lead to more efficient training of large language models, as the feedback can help guide the training process.\\n5. Better handling of complex tasks: The speaker believes that reinforcement learning with human feedback can be particularly effective in handling complex tasks that require a high degree of nuance and understanding.\\n6. Improved interpretability: The speaker notes that the use of reinforcement learning with human feedback can lead to more interpretable AI systems, as the feedback can help provide insights into how the systems are making decisions.\\n7. Enhanced safety and security: The speaker suggests that the use of reinforcement learning with human feedback can enhance safety and security in AI systems by allowing for more careful and deliberate decision-making.\\n8. Increased trust in AI: The speaker believes that the use of reinforcement learning with human feedback can increase trust in AI systems by providing a more transparent and accountable training process.\\n\\nOverall, the speaker seems to be highlighting the potential advantages of using reinforcement learning with human feedback to train large language models in AI, particularly in terms of improved accuracy, efficiency, and safety.')], 'answer': '\\nThe speaker is discussing the potential advantages of using reinforcement learning with human feedback to train large language models in AI. The speaker notes that this approach can provide several benefits, including:\\n\\n1. Exponential improvement: The speaker suggests that the use of reinforcement learning with human feedback can lead to an exponential improvement in the performance of large language models.\\n2. Democratization of AI: The speaker believes that the use of reinforcement learning with human feedback can democratize AI by allowing non-experts to contribute to the training process.\\n3. Improved accuracy: The speaker notes that the use of human feedback can lead to more accurate predictions and better decision-making in AI systems.\\n4. Increased efficiency: The speaker suggests that the use of reinforcement learning with human feedback can lead to more efficient training of large language models, as the feedback can help guide the training process.\\n5. Better handling of complex tasks: The speaker believes that reinforcement learning with human feedback can be particularly effective in handling complex tasks that require a high degree of nuance and understanding.\\n6. Improved interpretability: The speaker notes that the use of reinforcement learning with human feedback can lead to more interpretable AI systems, as the feedback can help provide insights into how the systems are making decisions.\\n7. Enhanced safety and security: The speaker suggests that the use of reinforcement learning with human feedback can enhance safety and security in AI systems by allowing for more careful and deliberate decision-making.\\n8. Increased trust in AI: The speaker believes that the use of reinforcement learning with human feedback can increase trust in AI systems by providing a more transparent and accountable training process.\\n\\nOverall, the speaker seems to be highlighting the potential advantages of using reinforcement learning with human feedback to train large language models in AI, particularly in terms of improved accuracy, efficiency, and safety.'}\n",
      " Standalone question: What are the potential downsides or limitations of using reinforcement learning with human feedback to train large language models in AI?</s>\n",
      "The potential downsides or limitations of using reinforcement learning with human feedback to train large language models in AI include:\n",
      "1. Bias and fairness: The training data used to reinforce the model may contain biases, which can result in biased or unfair outcomes.\n",
      "2. Lack of interpretability: The complex nature of reinforcement learning algorithms can make it difficult to interpret and understand the decisions made by the model.\n",
      "3. Limited domain knowledge: Reinforcement learning may not capture domain-specific knowledge and nuances, leading to suboptimal performance in certain tasks.\n",
      "4. Overfitting: The model may overfit the training data, resulting in poor generalization performance on unseen data.\n",
      "5. Lack of control: The model may not always behave as intended, especially when faced with unexpected or edge cases, due to the complex and non-linear nature of the reinforcement learning algorithm.\n",
      "6. Security risks: The use of human feedback to train the model may introduce security risks, such as data poisoning or manipulation, especially if the feedback is not properly validated or verified.\n",
      "7. Dependence on high-quality data: The quality of the training data has a significant impact on the performance of the model. If the data is poor or biased, the model may not perform well or make accurate predictions.\n",
      "8. Ethical considerations: The use of reinforcement learning with human feedback raises ethical concerns, such as the potential for mass manipulation or the impact on employment due to the automation of certain tasks.\n",
      "9. Lack of scalability: As the size of the model and the amount of training data increase, the computational requirements can become prohibitive, making it difficult to scale the model to larger sizes.\n",
      "10. Limited generalization: The model may not generalize well to new or unseen data, leading to poor performance in real-world scenarios.\n",
      "To mitigate these limitations, it is essential to carefully design and monitor the training process, use appropriate regularization techniques, and validate the model's performance on a variety of datasets and tasks. Additionally, it is crucial to consider the ethical implications of using reinforcement learning with human feedback and to ensure that the model is transparent, explainable, and fair.</s>\n",
      "{'question': 'What are the downsides of your last model suggested above ?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content=\"The speaker discusses the use of large language models in AI and their potential to revolutionize safety and security. They mention the importance of using these models in a way that is safe and secure, and how they can be trained to learn from human instructions. The speaker also discusses the potential for these models to be used in various domains, such as health, safety, and cybersecurity, and how they can be used to make people safer.\\n\\nIn terms of the question about models using human instructions, the speaker mentions that reinforcement learning with human feedback can be used to train these models. They explain that the user can provide feedback to the model, which can then improve its performance, making it more accurate and effective. The speaker also notes that this approach can democratize the use of AI, as people can contribute their expertise and knowledge to the model's training.\\n\\nOverall, the speaker emphasizes the potential of large language models in AI to revolutionize various fields and make a significant impact on society. They also highlight the importance of ensuring that these models are used in a responsible and ethical manner, with a focus on safety and security.\"), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content='\\nThe speaker is discussing the potential advantages of using reinforcement learning with human feedback to train large language models in AI. The speaker notes that this approach can provide several benefits, including:\\n\\n1. Exponential improvement: The speaker suggests that the use of reinforcement learning with human feedback can lead to an exponential improvement in the performance of large language models.\\n2. Democratization of AI: The speaker believes that the use of reinforcement learning with human feedback can democratize AI by allowing non-experts to contribute to the training process.\\n3. Improved accuracy: The speaker notes that the use of human feedback can lead to more accurate predictions and better decision-making in AI systems.\\n4. Increased efficiency: The speaker suggests that the use of reinforcement learning with human feedback can lead to more efficient training of large language models, as the feedback can help guide the training process.\\n5. Better handling of complex tasks: The speaker believes that reinforcement learning with human feedback can be particularly effective in handling complex tasks that require a high degree of nuance and understanding.\\n6. Improved interpretability: The speaker notes that the use of reinforcement learning with human feedback can lead to more interpretable AI systems, as the feedback can help provide insights into how the systems are making decisions.\\n7. Enhanced safety and security: The speaker suggests that the use of reinforcement learning with human feedback can enhance safety and security in AI systems by allowing for more careful and deliberate decision-making.\\n8. Increased trust in AI: The speaker believes that the use of reinforcement learning with human feedback can increase trust in AI systems by providing a more transparent and accountable training process.\\n\\nOverall, the speaker seems to be highlighting the potential advantages of using reinforcement learning with human feedback to train large language models in AI, particularly in terms of improved accuracy, efficiency, and safety.'), HumanMessage(content='What are the downsides of your last model suggested above ?'), AIMessage(content=\"The potential downsides or limitations of using reinforcement learning with human feedback to train large language models in AI include:\\n\\n1. Bias and fairness: The training data used to reinforce the model may contain biases, which can result in biased or unfair outcomes.\\n2. Lack of interpretability: The complex nature of reinforcement learning algorithms can make it difficult to interpret and understand the decisions made by the model.\\n3. Limited domain knowledge: Reinforcement learning may not capture domain-specific knowledge and nuances, leading to suboptimal performance in certain tasks.\\n4. Overfitting: The model may overfit the training data, resulting in poor generalization performance on unseen data.\\n5. Lack of control: The model may not always behave as intended, especially when faced with unexpected or edge cases, due to the complex and non-linear nature of the reinforcement learning algorithm.\\n6. Security risks: The use of human feedback to train the model may introduce security risks, such as data poisoning or manipulation, especially if the feedback is not properly validated or verified.\\n7. Dependence on high-quality data: The quality of the training data has a significant impact on the performance of the model. If the data is poor or biased, the model may not perform well or make accurate predictions.\\n8. Ethical considerations: The use of reinforcement learning with human feedback raises ethical concerns, such as the potential for mass manipulation or the impact on employment due to the automation of certain tasks.\\n9. Lack of scalability: As the size of the model and the amount of training data increase, the computational requirements can become prohibitive, making it difficult to scale the model to larger sizes.\\n10. Limited generalization: The model may not generalize well to new or unseen data, leading to poor performance in real-world scenarios.\\n\\nTo mitigate these limitations, it is essential to carefully design and monitor the training process, use appropriate regularization techniques, and validate the model's performance on a variety of datasets and tasks. Additionally, it is crucial to consider the ethical implications of using reinforcement learning with human feedback and to ensure that the model is transparent, explainable, and fair.\")], 'answer': \"The potential downsides or limitations of using reinforcement learning with human feedback to train large language models in AI include:\\n\\n1. Bias and fairness: The training data used to reinforce the model may contain biases, which can result in biased or unfair outcomes.\\n2. Lack of interpretability: The complex nature of reinforcement learning algorithms can make it difficult to interpret and understand the decisions made by the model.\\n3. Limited domain knowledge: Reinforcement learning may not capture domain-specific knowledge and nuances, leading to suboptimal performance in certain tasks.\\n4. Overfitting: The model may overfit the training data, resulting in poor generalization performance on unseen data.\\n5. Lack of control: The model may not always behave as intended, especially when faced with unexpected or edge cases, due to the complex and non-linear nature of the reinforcement learning algorithm.\\n6. Security risks: The use of human feedback to train the model may introduce security risks, such as data poisoning or manipulation, especially if the feedback is not properly validated or verified.\\n7. Dependence on high-quality data: The quality of the training data has a significant impact on the performance of the model. If the data is poor or biased, the model may not perform well or make accurate predictions.\\n8. Ethical considerations: The use of reinforcement learning with human feedback raises ethical concerns, such as the potential for mass manipulation or the impact on employment due to the automation of certain tasks.\\n9. Lack of scalability: As the size of the model and the amount of training data increase, the computational requirements can become prohibitive, making it difficult to scale the model to larger sizes.\\n10. Limited generalization: The model may not generalize well to new or unseen data, leading to poor performance in real-world scenarios.\\n\\nTo mitigate these limitations, it is essential to carefully design and monitor the training process, use appropriate regularization techniques, and validate the model's performance on a variety of datasets and tasks. Additionally, it is crucial to consider the ethical implications of using reinforcement learning with human feedback and to ensure that the model is transparent, explainable, and fair.\"}\n"
     ]
    }
   ],
   "source": [
    "print(llama_v2_chain({\"question\": \"What models use human instructions?\"}))\n",
    "\n",
    "print(llama_v2_chain({\"question\": \"Which are the advantage of each of these models?\"}))\n",
    "\n",
    "print(llama_v2_chain({\"question\": \"What are the downsides of your last model suggested above ?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc0e3a-36df-4f2d-bc5b-b8a2263aa58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## With ConversationalRetrievalChain 1 ############\n",
    "doc_chain_custom_prompt=\"\"\"Given the following extracted parts of a long document and a question, create a final answer with \"SOURCES\" that represent exactly the Source name and link given.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "{summaries}\n",
    "\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=doc_chain_custom_prompt, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "\n",
    "doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", prompt= qa_prompt, verbose=True)\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_index.as_retriever(search_kwargs={'k': 6}),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3191a48-aba5-42e8-b85f-afc886032ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## just load document chain question ############\n",
    "doc_chain_custom_template=\"\"\"Given chunks from multiple documents and a question, create an answer to the question that references those documents as \"SOURCES.\n",
    "\n",
    "- If the question asks about the system's capabilities, the system should respond with some version of \"This system can answer questions about building AI-powered products across the stack, about large language models, and the Full Stack's courses and materials.\". The answer does not need to include sources.\n",
    "- If the answer cannot be determined from the chunks or from these instructions, the system should not answer the question. The system should instead return \"No relevant sources found\".\n",
    "- Chunks are taken from the middle of documents and may be truncated or missing context.\n",
    "- Documents are not guaranteed to be relevant to the question.\n",
    "- Always include the \"SOURCES\" in your answer otherwise it is not valid.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{sources}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "doc_chain_custom_prompt = PromptTemplate(\n",
    "    template=doc_chain_custom_template, input_variables=[\"sources\", \"question\"]\n",
    ")\n",
    "\n",
    "import prompts\n",
    "load_doc_chain_chain = load_qa_with_sources_chain(\n",
    "        llm,\n",
    "        chain_type=\"stuff\",\n",
    "        verbose=True,\n",
    "        prompt=doc_chain_custom_prompt, # prompts.main,\n",
    "        document_variable_name=\"sources\",\n",
    "    )\n",
    "\n",
    "\n",
    "def sim_que(query : str):\n",
    "    pretty_log(\"selecting sources by similarity to query\")\n",
    "    sources_and_scores = vector_index.similarity_search_with_score(query, k=3)\n",
    "\n",
    "    sources, scores = zip(*sources_and_scores)\n",
    "    #print(sources_and_scores)\n",
    "\n",
    "\n",
    "    result = load_doc_chain_chain(\n",
    "            {\"input_documents\": sources, \"question\": query}, return_only_outputs=True\n",
    "        )\n",
    "\n",
    "    answer = result[\"output_text\"]\n",
    "    print(answer)\n",
    "\n",
    "query1 = \"What models use human instructions?\"\n",
    "sim_que(query1)\n",
    "\n",
    "query2 = \"Are there any model trained on medical knowledge?\"\n",
    "sim_que(query2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2687e6-318f-4308-806f-6e37b1a49097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# doc_chain({\"question\":\"What models use human instructions?\", \"input_documents\":sources})\n",
    "\n",
    "chain({\"question\": \"What models use human instructions?\"})\n",
    "\n",
    "chain({\"question\": \"Which are the advantage of each of these models?\"})\n",
    "\n",
    "chain({\"question\": \"Can you elaborate more on point 3?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4b6c6-0e80-46f8-a0fa-f90cdf34cb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain({\"question\":\"Which are the advantage of each of these models?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee460ef-2914-42bf-8aab-dda8955dce1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.qanda_llama2_cont()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ea16b-407a-4068-9ba1-7b194f17fef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "chat_history = \"\"\n",
    "input_list = [{\"user_input\": \"Give me some indications to solve a denial of service attack.\", \"chat_history\":chat_history}]\n",
    "\n",
    "start=time.time()\n",
    "test.llm_chain.generate(input_list)\n",
    "\n",
    "test.llm_chain.generate([{\"user_input\" : \"What question did I asked you previously\"}])\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa58a52-4f28-428f-afbb-72efc29906dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "chat_history = \"\"\n",
    "input_list = [{\"user_input\": \"Give me some indications to solve a denial of service attack.\", \"chat_history\":chat_history}]\n",
    "\n",
    "start=time.time()\n",
    "answer1=test.llm_chain.predict(user_input=\"Give me some indications to solve a denial of service attack.\")\n",
    "print(answer1)\n",
    "answer2=test.llm_chain.predict(user_input=\"What question did I asked you previously\")\n",
    "print(answer2)\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ba867-e64e-4b3c-bd20-5d0ca35f91c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = test.qanda_llama2(\"Can we combine LMMs and OCR?\", with_logging=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bbb003be7165b",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = test.qanda_llama2_withRAG(\"Can we combine LMMs and OCR?\", with_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1b24d-2ff9-42b2-a46b-c0fc385943aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test.qanda_llama2_withRAG(\"Can we combine LLMs and OCR\", with_logging=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c76322-f71b-4272-9207-52c1faf46ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test.ask_question_withRAG(\"Can we combine LLMs and OCR\", with_logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a191cb-186f-4250-95ba-7f956df150ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema.runnable  import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable.utils import ConfigurableField\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4a6e4-f899-45d1-ad8d-d0511e7c9f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=test.embedding_engine)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "llm=test.base_llm\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "########### \n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use 4 sentences maximum and keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),        \n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d6e2b-e96c-4fad-8342-333d9a10d164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_chain_with_source.invoke({question:\"What is Task Decomposition?\"})#\"Always say ```thanks for asking!``` at the end of the answer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f252f-210a-4725-b735-7200f1b9e9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    prompt.invoke(\n",
    "        {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a354bc-c945-4304-a996-ff835b5c04b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "streamer = TextStreamer(test.tokenizer, skip_prompt=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=test.base_llm,\n",
    "    tokenizer=test.tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=0.6,\n",
    "    pad_token_id=test.tokenizer.eos_token_id,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    device=0,\n",
    "    streamer=streamer\n",
    ")\n",
    "pipe(prompts[0])\n",
    "\n",
    "inputs = test.tokenizer(prompts[0], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model_fintuned.generate(**inputs, streamer=streamer, pad_token_id=tokenizer.eos_token_id, max_length=248, temperature=0.8, top_p=0.8,\n",
    "                        repetition_penalty=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cc850-e0ec-4697-b4e9-15e56dea3666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
