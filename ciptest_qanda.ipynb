{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b7eef9-23a4-430a-bc89-d3002293e276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import secrets\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pdb\n",
    "from typing import Any\n",
    "\n",
    "from etl import markdown, pdfs, shared, videos\n",
    "from etl.shared import display_modal_image\n",
    "\n",
    "import docstore\n",
    "import vecstore\n",
    "from utils import pretty_log\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import textwrap\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import langchain\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "import ciptest_qanda as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b544796-7749-4ab5-888d-476eb2feb652",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9253a1c3365e43eaaeb687c0f45a3bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mðŸ¥ž: connecting to vector storage \u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open vectors/openai-ada-fsdl.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_589676/3277643737.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqa_with_sources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_qa_with_sources_chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversational_retrieval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mpretty_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connecting to vector storage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mvector_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvecstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_vector_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINDEX_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mpretty_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connected to vector storage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpretty_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"found {vector_index.index.ntotal} vectors to search over\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dynabicChatbot/vecstore.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(index_name, embedding_engine)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconnect_to_vector_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"\"\"Adds the texts and metadatas to the vector index.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mvector_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVECTOR_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvector_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, folder_path, embeddings, index_name, **kwargs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \"\"\"\n\u001b[1;32m   1054\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         index = faiss.read_index(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"{index_name}.faiss\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         )\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  10205\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10206\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open vectors/openai-ada-fsdl.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import textwrap\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "embedding_engine = vecstore.get_embedding_engine(allowed_special=\"all\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                          token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             token=True,\n",
    "                                             #  load_in_8bit=True,\n",
    "                                             #  load_in_4bit=True,\n",
    "                                             )\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens=4096,\n",
    "                do_sample=True,\n",
    "                # temperature=0.1,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                streamer=streamer,\n",
    "                )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "pretty_log(\"connecting to vector storage\")\n",
    "vector_index = vecstore.connect_to_vector_index(vecstore.INDEX_NAME, embedding_engine)\n",
    "pretty_log(\"connected to vector storage\")\n",
    "pretty_log(f\"found {vector_index.index.ntotal} vectors to search over\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45dd9d8-6f8f-4805-9935-664c46317b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## THE LAMA 2 DEMO ##############\n",
    "\n",
    "template = \"\"\"\n",
    "[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n",
    "{context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "retriever = vector_index.as_retriever(\n",
    "        search_kwargs={\"k\": 6}\n",
    "    )\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,     \n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e5a85-5a0e-48dc-a0f1-2f0f777080fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"What models use human instructions?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "591e0b55-dedb-4553-b286-3dd1ac943eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## THE LAMA 2 DEMO ############## - MORE GENERIC AND CUSTOMIZED \n",
    "langchain.debug=False \n",
    "\n",
    "llama_docs_template = \"\"\"\n",
    "[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n",
    "{context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "llama_docs_prompt = PromptTemplate(template=llama_docs_template, input_variables=[\"context\", \"question\"])\n",
    "llama_doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", prompt= llama_docs_prompt, document_variable_name=\"context\", verbose=False)\n",
    "\n",
    "llama_condense_template = \"\"\"\n",
    "[INST]Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question: [/INST]\"\"\"\n",
    "llama_condense_prompt = PromptTemplate(template=llama_condense_template, input_variables=[\"chat_history\", \"question\"])\n",
    "llama_question_generator_chain = LLMChain(llm=llm, prompt=llama_condense_prompt, verbose=False)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llama_v2_chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_index.as_retriever(search_kwargs={'k': 6}),\n",
    "    question_generator=llama_question_generator_chain,\n",
    "    combine_docs_chain=llama_doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae01051-b5ea-47b2-a521-291ed6771156",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Which models use human instructions?\n",
      "\n",
      "According to the conversation, the following models use human instructions:\n",
      "\n",
      "1. LLaV A\n",
      "2. MiniGPT4\n",
      "\n",
      "These models leverage human instructions to improve their performance in various visual understanding and reasoning tasks, such as text recognition, TextVQA, KIE, and HMER.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the provided context, the following models use human instructions:\n",
      "\n",
      "1. LLaV A: This model utilizes a multimodal instruction-tuned dataset to train the language model and projection layer, which enables it to complete various visual understanding and reasoning tasks.\n",
      "2. MiniGPT4: This model shares a similar network architecture with LLaV A but employs ViT-G/14 and Q-Former to encode images. It only updates the projection layer on a combined dataset of 5 million image-text pairs sampled from LAION, Conceptual Captions, and SBU, and then on 3.5K high-quality conversational data to enhance the naturalness and usability of the generated responses.\n",
      "\n",
      "Both LLaV A and MiniGPT4 leverage human instructions to improve their performance in various visual understanding and reasoning tasks.</s>\n",
      "{'question': 'What models use human instructions?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content='Based on the provided text, the following models use human instructions:\\n\\n1. LLaV A: This model utilizes self-instruct [27] with language-only GPT-4 to generate a high-quality multimodal language-image instruction-following dataset with 158K instances.\\n2. MiniGPT4: This model employs ViT-G/14 and Q-Former [16] to encode images and uses human-generated instructions for training.\\n3. mPLUG-Owl: This model integrates unimodal and multimodal data during training, using a combination of human-generated instructions and large-scale image-text pairs.\\n\\nThese models leverage human instructions to improve their performance in various visual understanding and reasoning tasks, such as text recognition, TextVQA, KIE, and HMER.'), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content=\"The advantages of each of the models mentioned in the article are as follows:\\n\\n1. BLIP-2 OPT:\\n* Has a large-scale pre-training dataset of 6.7 billion words\\n* Uses a multi-modal transformer architecture that can handle both text and image inputs\\n* Demonstrates state-of-the-art performance on various downstream tasks such as text classification, question answering, and visual grounding\\n2. BLIP-2 FlanT5 XXL:\\n* Utilizes a larger transformer size and more training data than BLIP-2 OPT\\n* Shows improved performance on some downstream tasks compared to BLIP-2 OPT\\n* Demonstrates better handling of long-range dependencies and better text-to-image alignment\\n3. OpenFlamingo:\\n* Uses a modular architecture that separates the language model and the image-text matching module\\n* Employs a smaller language model compared to BLIP-2 OPT, which can lead to more efficient training and deployment\\n* Demonstrates strong performance on various downstream tasks such as text classification, question answering, and visual grounding\\n4. LLaV A:\\n* Utilizes a smaller dataset of 595K image-text pairs for training compared to other models\\n* Demonstrates strong performance on various downstream tasks such as text classification, question answering, and visual grounding\\n* Shows better handling of out-of-vocabulary words compared to other models\\n5. MiniGPT4:\\n* Uses a smaller language model compared to BLIP-2 OPT and demonstrates similar performance on some downstream tasks\\n* Employs a larger transformer size compared to BLIP-2 FlanT5 XXL and demonstrates improved performance on some downstream tasks\\n* Demonstrates better handling of long-range dependencies compared to other models\\n\\nIt's worth noting that the advantages of each model can vary depending on the specific task and dataset being used, and that the performance of each model can also be influenced by factors such as the quality of the training data and the specific implementation details.\"), HumanMessage(content='Can you elaborate more last point?'), AIMessage(content=\"The performance of each model in visual understanding and reasoning tasks can be influenced by various factors, including the type of data used for training, the model's architecture and training parameters, and the specific task or dataset being evaluated. Here are some potential factors that can impact the performance of each model:\\n\\n1. Data quality and diversity: The quality and diversity of the training data can significantly impact the performance of each model. For instance, if the training data is limited to a specific domain or genre, the model may struggle to generalize to other types of data. Similarly, if the training data is noisy or contains biases, the model may not perform well on clean or unbiased data.\\n2. Model architecture and hyperparameters: The design of the model's architecture and the choice of hyperparameters can also impact its performance. For example, a model with a larger number of layers or a larger receptive field may be better suited for tasks that require long-range dependencies, while a model with a smaller number of layers or a smaller receptive field may be more efficient for tasks that require localized features.\\n3. Task complexity: The complexity of the task can also influence the performance of each model. For instance, tasks that require a higher level of cognitive reasoning, such as multi-step reasoning or common sense reasoning, may be more challenging for some models than tasks that require simpler reasoning, such as keyword extraction or sentiment analysis.\\n4. Dataset size and diversity: The size and diversity of the dataset can also impact the performance of each model. A larger dataset with a more diverse range of examples can provide more opportunities for the model to learn and generalize, leading to better performance.\\n5. Training strategy and optimization: The training strategy and optimization methods used can also impact the performance of each model. For example, using transfer learning or pre-training can help the model learn general features that are useful for a wide range of tasks, while using more advanced optimization methods, such as Adam or RMSProp, can help the model converge faster and to a better local minimum.\\n6. Model's ability to handle occlusions and irregularities: The ability of each model to handle occlusions and irregularities in the input data can also impact its performance. For example, a model that is better able to handle occlusions may perform better on tasks that require identifying objects or text in cluttered scenes.\\n7. Model's ability to handle long-range dependencies: The ability of each model to handle long-range dependencies can also impact its performance. For example, a model that is better able to handle long-range dependencies may perform better on tasks that require identifying relationships between distant objects or concepts.\\n8. Model's ability to handle multimodal input: The ability of each model to handle multimodal input, such as images and text, can also impact its performance. For example, a model that is better able to integrate visual and textual information may perform better on tasks that require a combination of both modalities.\\n\\nOverall, the factors that influence the performance of each model in visual understanding and reasoning tasks can be complex and interrelated. Understanding these factors can help researchers and practitioners optimize their models and improve their performance on these tasks.\"), HumanMessage(content='What models use human instructions?'), AIMessage(content='\\nBased on the provided context, the following models use human instructions:\\n\\n1. LLaV A: This model utilizes a multimodal instruction-tuned dataset to train the language model and projection layer, which enables it to complete various visual understanding and reasoning tasks.\\n2. MiniGPT4: This model shares a similar network architecture with LLaV A but employs ViT-G/14 and Q-Former to encode images. It only updates the projection layer on a combined dataset of 5 million image-text pairs sampled from LAION, Conceptual Captions, and SBU, and then on 3.5K high-quality conversational data to enhance the naturalness and usability of the generated responses.\\n\\nBoth LLaV A and MiniGPT4 leverage human instructions to improve their performance in various visual understanding and reasoning tasks.')], 'answer': '\\nBased on the provided context, the following models use human instructions:\\n\\n1. LLaV A: This model utilizes a multimodal instruction-tuned dataset to train the language model and projection layer, which enables it to complete various visual understanding and reasoning tasks.\\n2. MiniGPT4: This model shares a similar network architecture with LLaV A but employs ViT-G/14 and Q-Former to encode images. It only updates the projection layer on a combined dataset of 5 million image-text pairs sampled from LAION, Conceptual Captions, and SBU, and then on 3.5K high-quality conversational data to enhance the naturalness and usability of the generated responses.\\n\\nBoth LLaV A and MiniGPT4 leverage human instructions to improve their performance in various visual understanding and reasoning tasks.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Which are the advantages of each of the models that use human instructions?</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the given context, the advantages of each of the models that use human instructions are:\n",
      "\n",
      "1. LLaV A: LLaV A uses instruction-following data to enhance the naturalness and usability of the generated responses. This allows the model to generate more accurate and relevant responses to given prompts.\n",
      "2. MiniGPT4: MiniGPT4 employs ViT-G/14 and Q-Former in [16] to encode images. This allows the model to generate more accurate and relevant responses to given prompts.\n",
      "3. mPLUG-Owl: mPLUG-Owl integrates unimodal and multimodal data during training. This allows the model to learn a more robust representation of the input data, leading to better performance on OCR tasks.\n",
      "4. BLIP-2: BLIP-2 uses a combination of regular, irregular, occluded, and others to train the model. This allows the model to learn a more robust representation of the input data, leading to better performance on OCR tasks.\n",
      "\n",
      "In general, the use of human instructions in these models allows them to better understand the context and intent behind a given prompt, leading to more accurate and relevant responses.</s>\n",
      "{'question': 'Which are the advantage of each of these models?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content='Based on the provided text, the following models use human instructions:\\n\\n1. LLaV A: This model utilizes self-instruct [27] with language-only GPT-4 to generate a high-quality multimodal language-image instruction-following dataset with 158K instances.\\n2. MiniGPT4: This model employs ViT-G/14 and Q-Former [16] to encode images and uses human-generated instructions for training.\\n3. mPLUG-Owl: This model integrates unimodal and multimodal data during training, using a combination of human-generated instructions and large-scale image-text pairs.\\n\\nThese models leverage human instructions to improve their performance in various visual understanding and reasoning tasks, such as text recognition, TextVQA, KIE, and HMER.'), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content=\"The advantages of each of the models mentioned in the article are as follows:\\n\\n1. BLIP-2 OPT:\\n* Has a large-scale pre-training dataset of 6.7 billion words\\n* Uses a multi-modal transformer architecture that can handle both text and image inputs\\n* Demonstrates state-of-the-art performance on various downstream tasks such as text classification, question answering, and visual grounding\\n2. BLIP-2 FlanT5 XXL:\\n* Utilizes a larger transformer size and more training data than BLIP-2 OPT\\n* Shows improved performance on some downstream tasks compared to BLIP-2 OPT\\n* Demonstrates better handling of long-range dependencies and better text-to-image alignment\\n3. OpenFlamingo:\\n* Uses a modular architecture that separates the language model and the image-text matching module\\n* Employs a smaller language model compared to BLIP-2 OPT, which can lead to more efficient training and deployment\\n* Demonstrates strong performance on various downstream tasks such as text classification, question answering, and visual grounding\\n4. LLaV A:\\n* Utilizes a smaller dataset of 595K image-text pairs for training compared to other models\\n* Demonstrates strong performance on various downstream tasks such as text classification, question answering, and visual grounding\\n* Shows better handling of out-of-vocabulary words compared to other models\\n5. MiniGPT4:\\n* Uses a smaller language model compared to BLIP-2 OPT and demonstrates similar performance on some downstream tasks\\n* Employs a larger transformer size compared to BLIP-2 FlanT5 XXL and demonstrates improved performance on some downstream tasks\\n* Demonstrates better handling of long-range dependencies compared to other models\\n\\nIt's worth noting that the advantages of each model can vary depending on the specific task and dataset being used, and that the performance of each model can also be influenced by factors such as the quality of the training data and the specific implementation details.\"), HumanMessage(content='Can you elaborate more last point?'), AIMessage(content=\"The performance of each model in visual understanding and reasoning tasks can be influenced by various factors, including the type of data used for training, the model's architecture and training parameters, and the specific task or dataset being evaluated. Here are some potential factors that can impact the performance of each model:\\n\\n1. Data quality and diversity: The quality and diversity of the training data can significantly impact the performance of each model. For instance, if the training data is limited to a specific domain or genre, the model may struggle to generalize to other types of data. Similarly, if the training data is noisy or contains biases, the model may not perform well on clean or unbiased data.\\n2. Model architecture and hyperparameters: The design of the model's architecture and the choice of hyperparameters can also impact its performance. For example, a model with a larger number of layers or a larger receptive field may be better suited for tasks that require long-range dependencies, while a model with a smaller number of layers or a smaller receptive field may be more efficient for tasks that require localized features.\\n3. Task complexity: The complexity of the task can also influence the performance of each model. For instance, tasks that require a higher level of cognitive reasoning, such as multi-step reasoning or common sense reasoning, may be more challenging for some models than tasks that require simpler reasoning, such as keyword extraction or sentiment analysis.\\n4. Dataset size and diversity: The size and diversity of the dataset can also impact the performance of each model. A larger dataset with a more diverse range of examples can provide more opportunities for the model to learn and generalize, leading to better performance.\\n5. Training strategy and optimization: The training strategy and optimization methods used can also impact the performance of each model. For example, using transfer learning or pre-training can help the model learn general features that are useful for a wide range of tasks, while using more advanced optimization methods, such as Adam or RMSProp, can help the model converge faster and to a better local minimum.\\n6. Model's ability to handle occlusions and irregularities: The ability of each model to handle occlusions and irregularities in the input data can also impact its performance. For example, a model that is better able to handle occlusions may perform better on tasks that require identifying objects or text in cluttered scenes.\\n7. Model's ability to handle long-range dependencies: The ability of each model to handle long-range dependencies can also impact its performance. For example, a model that is better able to handle long-range dependencies may perform better on tasks that require identifying relationships between distant objects or concepts.\\n8. Model's ability to handle multimodal input: The ability of each model to handle multimodal input, such as images and text, can also impact its performance. For example, a model that is better able to integrate visual and textual information may perform better on tasks that require a combination of both modalities.\\n\\nOverall, the factors that influence the performance of each model in visual understanding and reasoning tasks can be complex and interrelated. Understanding these factors can help researchers and practitioners optimize their models and improve their performance on these tasks.\"), HumanMessage(content='What models use human instructions?'), AIMessage(content='\\nBased on the provided context, the following models use human instructions:\\n\\n1. LLaV A: This model utilizes a multimodal instruction-tuned dataset to train the language model and projection layer, which enables it to complete various visual understanding and reasoning tasks.\\n2. MiniGPT4: This model shares a similar network architecture with LLaV A but employs ViT-G/14 and Q-Former to encode images. It only updates the projection layer on a combined dataset of 5 million image-text pairs sampled from LAION, Conceptual Captions, and SBU, and then on 3.5K high-quality conversational data to enhance the naturalness and usability of the generated responses.\\n\\nBoth LLaV A and MiniGPT4 leverage human instructions to improve their performance in various visual understanding and reasoning tasks.'), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content='\\nBased on the given context, the advantages of each of the models that use human instructions are:\\n\\n1. LLaV A: LLaV A uses instruction-following data to enhance the naturalness and usability of the generated responses. This allows the model to generate more accurate and relevant responses to given prompts.\\n2. MiniGPT4: MiniGPT4 employs ViT-G/14 and Q-Former in [16] to encode images. This allows the model to generate more accurate and relevant responses to given prompts.\\n3. mPLUG-Owl: mPLUG-Owl integrates unimodal and multimodal data during training. This allows the model to learn a more robust representation of the input data, leading to better performance on OCR tasks.\\n4. BLIP-2: BLIP-2 uses a combination of regular, irregular, occluded, and others to train the model. This allows the model to learn a more robust representation of the input data, leading to better performance on OCR tasks.\\n\\nIn general, the use of human instructions in these models allows them to better understand the context and intent behind a given prompt, leading to more accurate and relevant responses.')], 'answer': '\\nBased on the given context, the advantages of each of the models that use human instructions are:\\n\\n1. LLaV A: LLaV A uses instruction-following data to enhance the naturalness and usability of the generated responses. This allows the model to generate more accurate and relevant responses to given prompts.\\n2. MiniGPT4: MiniGPT4 employs ViT-G/14 and Q-Former in [16] to encode images. This allows the model to generate more accurate and relevant responses to given prompts.\\n3. mPLUG-Owl: mPLUG-Owl integrates unimodal and multimodal data during training. This allows the model to learn a more robust representation of the input data, leading to better performance on OCR tasks.\\n4. BLIP-2: BLIP-2 uses a combination of regular, irregular, occluded, and others to train the model. This allows the model to learn a more robust representation of the input data, leading to better performance on OCR tasks.\\n\\nIn general, the use of human instructions in these models allows them to better understand the context and intent behind a given prompt, leading to more accurate and relevant responses.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The downsides of the last model suggested (LLaV A) are:\n",
      "\n",
      "1. Limited dataset size: LLaV A utilizes a small dataset of 595K image-text pairs for training, which may limit its performance on tasks that require a larger amount of data.\n",
      "2. Modular architecture: LLaV A's modular architecture can make it more difficult to optimize and train, as the language model and image-text matching module may have different optimization goals and requirements.\n",
      "3. Dependence on high-quality instructions: LLaV A's performance relies heavily on the quality and relevance of the instructions provided. If the instructions are noisy or biased, the model may not perform well.\n",
      "4. Limited generalization: LLaV A is trained on a specific dataset and may not generalize well to other datasets or tasks.\n",
      "5. Requires careful tuning: LLaV A requires careful tuning of the hyperparameters and training parameters to achieve good performance. This can be time-consuming and may require expert knowledge.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the performance of large multimodal models (LMMs) in various tasks, including text recognition, visual question answering, key information extraction, and handwritten mathematical expression recognition. The study finds that while LMMs show strengths in text recognition and comprehension tasks, they struggle with fine-grained feature detection in images and recognize certain text images as more common words based on semantics. The article also highlights the limitations of LLaV A, a multimodal instruction-tuned model, including its dependence on high-quality instructions, limited generalization, and requirement for careful tuning.\n",
      "\n",
      "Based on the findings, the article concludes that LMMs work for OCR due to their pre-trained visual encoder and Large Language Model, which operate within their structured feature space. However, unsupervised text-image pairs in the training data cannot compete with fully supervised data. The article suggests that adapting LMMs to the OCR domain can improve their performance.\n",
      "\n",
      "In summary, the article provides insights into the strengths and limitations of LMMs in various tasks and highlights the potential for improving their performance in OCR by adapting them to the domain.</s>\n",
      "{'question': 'What are the downsides of your last model suggested above ?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content='Based on the provided text, the following models use human instructions:\\n\\n1. LLaV A: This model utilizes self-instruct [27] with language-only GPT-4 to generate a high-quality multimodal language-image instruction-following dataset with 158K instances.\\n2. MiniGPT4: This model employs ViT-G/14 and Q-Former [16] to encode images and uses human-generated instructions for training.\\n3. mPLUG-Owl: This model integrates unimodal and multimodal data during training, using a combination of human-generated instructions and large-scale image-text pairs.\\n\\nThese models leverage human instructions to improve their performance in various visual understanding and reasoning tasks, such as text recognition, TextVQA, KIE, and HMER.'), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content=\"The advantages of each of the models mentioned in the article are as follows:\\n\\n1. BLIP-2 OPT:\\n* Has a large-scale pre-training dataset of 6.7 billion words\\n* Uses a multi-modal transformer architecture that can handle both text and image inputs\\n* Demonstrates state-of-the-art performance on various downstream tasks such as text classification, question answering, and visual grounding\\n2. BLIP-2 FlanT5 XXL:\\n* Utilizes a larger transformer size and more training data than BLIP-2 OPT\\n* Shows improved performance on some downstream tasks compared to BLIP-2 OPT\\n* Demonstrates better handling of long-range dependencies and better text-to-image alignment\\n3. OpenFlamingo:\\n* Uses a modular architecture that separates the language model and the image-text matching module\\n* Employs a smaller language model compared to BLIP-2 OPT, which can lead to more efficient training and deployment\\n* Demonstrates strong performance on various downstream tasks such as text classification, question answering, and visual grounding\\n4. LLaV A:\\n* Utilizes a smaller dataset of 595K image-text pairs for training compared to other models\\n* Demonstrates strong performance on various downstream tasks such as text classification, question answering, and visual grounding\\n* Shows better handling of out-of-vocabulary words compared to other models\\n5. MiniGPT4:\\n* Uses a smaller language model compared to BLIP-2 OPT and demonstrates similar performance on some downstream tasks\\n* Employs a larger transformer size compared to BLIP-2 FlanT5 XXL and demonstrates improved performance on some downstream tasks\\n* Demonstrates better handling of long-range dependencies compared to other models\\n\\nIt's worth noting that the advantages of each model can vary depending on the specific task and dataset being used, and that the performance of each model can also be influenced by factors such as the quality of the training data and the specific implementation details.\"), HumanMessage(content='Can you elaborate more last point?'), AIMessage(content=\"The performance of each model in visual understanding and reasoning tasks can be influenced by various factors, including the type of data used for training, the model's architecture and training parameters, and the specific task or dataset being evaluated. Here are some potential factors that can impact the performance of each model:\\n\\n1. Data quality and diversity: The quality and diversity of the training data can significantly impact the performance of each model. For instance, if the training data is limited to a specific domain or genre, the model may struggle to generalize to other types of data. Similarly, if the training data is noisy or contains biases, the model may not perform well on clean or unbiased data.\\n2. Model architecture and hyperparameters: The design of the model's architecture and the choice of hyperparameters can also impact its performance. For example, a model with a larger number of layers or a larger receptive field may be better suited for tasks that require long-range dependencies, while a model with a smaller number of layers or a smaller receptive field may be more efficient for tasks that require localized features.\\n3. Task complexity: The complexity of the task can also influence the performance of each model. For instance, tasks that require a higher level of cognitive reasoning, such as multi-step reasoning or common sense reasoning, may be more challenging for some models than tasks that require simpler reasoning, such as keyword extraction or sentiment analysis.\\n4. Dataset size and diversity: The size and diversity of the dataset can also impact the performance of each model. A larger dataset with a more diverse range of examples can provide more opportunities for the model to learn and generalize, leading to better performance.\\n5. Training strategy and optimization: The training strategy and optimization methods used can also impact the performance of each model. For example, using transfer learning or pre-training can help the model learn general features that are useful for a wide range of tasks, while using more advanced optimization methods, such as Adam or RMSProp, can help the model converge faster and to a better local minimum.\\n6. Model's ability to handle occlusions and irregularities: The ability of each model to handle occlusions and irregularities in the input data can also impact its performance. For example, a model that is better able to handle occlusions may perform better on tasks that require identifying objects or text in cluttered scenes.\\n7. Model's ability to handle long-range dependencies: The ability of each model to handle long-range dependencies can also impact its performance. For example, a model that is better able to handle long-range dependencies may perform better on tasks that require identifying relationships between distant objects or concepts.\\n8. Model's ability to handle multimodal input: The ability of each model to handle multimodal input, such as images and text, can also impact its performance. For example, a model that is better able to integrate visual and textual information may perform better on tasks that require a combination of both modalities.\\n\\nOverall, the factors that influence the performance of each model in visual understanding and reasoning tasks can be complex and interrelated. Understanding these factors can help researchers and practitioners optimize their models and improve their performance on these tasks.\"), HumanMessage(content='What models use human instructions?'), AIMessage(content='\\nBased on the provided context, the following models use human instructions:\\n\\n1. LLaV A: This model utilizes a multimodal instruction-tuned dataset to train the language model and projection layer, which enables it to complete various visual understanding and reasoning tasks.\\n2. MiniGPT4: This model shares a similar network architecture with LLaV A but employs ViT-G/14 and Q-Former to encode images. It only updates the projection layer on a combined dataset of 5 million image-text pairs sampled from LAION, Conceptual Captions, and SBU, and then on 3.5K high-quality conversational data to enhance the naturalness and usability of the generated responses.\\n\\nBoth LLaV A and MiniGPT4 leverage human instructions to improve their performance in various visual understanding and reasoning tasks.'), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content='\\nBased on the given context, the advantages of each of the models that use human instructions are:\\n\\n1. LLaV A: LLaV A uses instruction-following data to enhance the naturalness and usability of the generated responses. This allows the model to generate more accurate and relevant responses to given prompts.\\n2. MiniGPT4: MiniGPT4 employs ViT-G/14 and Q-Former in [16] to encode images. This allows the model to generate more accurate and relevant responses to given prompts.\\n3. mPLUG-Owl: mPLUG-Owl integrates unimodal and multimodal data during training. This allows the model to learn a more robust representation of the input data, leading to better performance on OCR tasks.\\n4. BLIP-2: BLIP-2 uses a combination of regular, irregular, occluded, and others to train the model. This allows the model to learn a more robust representation of the input data, leading to better performance on OCR tasks.\\n\\nIn general, the use of human instructions in these models allows them to better understand the context and intent behind a given prompt, leading to more accurate and relevant responses.'), HumanMessage(content='What are the downsides of your last model suggested above ?'), AIMessage(content='The article discusses the performance of large multimodal models (LMMs) in various tasks, including text recognition, visual question answering, key information extraction, and handwritten mathematical expression recognition. The study finds that while LMMs show strengths in text recognition and comprehension tasks, they struggle with fine-grained feature detection in images and recognize certain text images as more common words based on semantics. The article also highlights the limitations of LLaV A, a multimodal instruction-tuned model, including its dependence on high-quality instructions, limited generalization, and requirement for careful tuning.\\n\\nBased on the findings, the article concludes that LMMs work for OCR due to their pre-trained visual encoder and Large Language Model, which operate within their structured feature space. However, unsupervised text-image pairs in the training data cannot compete with fully supervised data. The article suggests that adapting LMMs to the OCR domain can improve their performance.\\n\\nIn summary, the article provides insights into the strengths and limitations of LMMs in various tasks and highlights the potential for improving their performance in OCR by adapting them to the domain.')], 'answer': 'The article discusses the performance of large multimodal models (LMMs) in various tasks, including text recognition, visual question answering, key information extraction, and handwritten mathematical expression recognition. The study finds that while LMMs show strengths in text recognition and comprehension tasks, they struggle with fine-grained feature detection in images and recognize certain text images as more common words based on semantics. The article also highlights the limitations of LLaV A, a multimodal instruction-tuned model, including its dependence on high-quality instructions, limited generalization, and requirement for careful tuning.\\n\\nBased on the findings, the article concludes that LMMs work for OCR due to their pre-trained visual encoder and Large Language Model, which operate within their structured feature space. However, unsupervised text-image pairs in the training data cannot compete with fully supervised data. The article suggests that adapting LMMs to the OCR domain can improve their performance.\\n\\nIn summary, the article provides insights into the strengths and limitations of LMMs in various tasks and highlights the potential for improving their performance in OCR by adapting them to the domain.'}\n"
     ]
    }
   ],
   "source": [
    "print(llama_v2_chain({\"question\": \"What models use human instructions?\"}))\n",
    "\n",
    "print(llama_v2_chain({\"question\": \"Which are the advantage of each of these models?\"}))\n",
    "\n",
    "print(llama_v2_chain({\"question\": \"What are the downsides of your last model suggested above ?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc0e3a-36df-4f2d-bc5b-b8a2263aa58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## With ConversationalRetrievalChain 1 ############\n",
    "doc_chain_custom_prompt=\"\"\"Given the following extracted parts of a long document and a question, create a final answer with \"SOURCES\" that represent exactly the Source name and link given.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "{summaries}\n",
    "\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=doc_chain_custom_prompt, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "\n",
    "doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", prompt= qa_prompt, verbose=True)\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_index.as_retriever(search_kwargs={'k': 6}),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3191a48-aba5-42e8-b85f-afc886032ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## just load document chain question ############\n",
    "doc_chain_custom_template=\"\"\"Given chunks from multiple documents and a question, create an answer to the question that references those documents as \"SOURCES.\n",
    "\n",
    "- If the question asks about the system's capabilities, the system should respond with some version of \"This system can answer questions about building AI-powered products across the stack, about large language models, and the Full Stack's courses and materials.\". The answer does not need to include sources.\n",
    "- If the answer cannot be determined from the chunks or from these instructions, the system should not answer the question. The system should instead return \"No relevant sources found\".\n",
    "- Chunks are taken from the middle of documents and may be truncated or missing context.\n",
    "- Documents are not guaranteed to be relevant to the question.\n",
    "- Always include the \"SOURCES\" in your answer otherwise it is not valid.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{sources}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "doc_chain_custom_prompt = PromptTemplate(\n",
    "    template=doc_chain_custom_template, input_variables=[\"sources\", \"question\"]\n",
    ")\n",
    "\n",
    "import prompts\n",
    "load_doc_chain_chain = load_qa_with_sources_chain(\n",
    "        llm,\n",
    "        chain_type=\"stuff\",\n",
    "        verbose=True,\n",
    "        prompt=doc_chain_custom_prompt, # prompts.main,\n",
    "        document_variable_name=\"sources\",\n",
    "    )\n",
    "\n",
    "\n",
    "def sim_que(query : str):\n",
    "    pretty_log(\"selecting sources by similarity to query\")\n",
    "    sources_and_scores = vector_index.similarity_search_with_score(query, k=3)\n",
    "\n",
    "    sources, scores = zip(*sources_and_scores)\n",
    "    #print(sources_and_scores)\n",
    "\n",
    "\n",
    "    result = load_doc_chain_chain(\n",
    "            {\"input_documents\": sources, \"question\": query}, return_only_outputs=True\n",
    "        )\n",
    "\n",
    "    answer = result[\"output_text\"]\n",
    "    print(answer)\n",
    "\n",
    "query1 = \"What models use human instructions?\"\n",
    "sim_que(query1)\n",
    "\n",
    "query2 = \"Are there any model trained on medical knowledge?\"\n",
    "sim_que(query2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2687e6-318f-4308-806f-6e37b1a49097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# doc_chain({\"question\":\"What models use human instructions?\", \"input_documents\":sources})\n",
    "\n",
    "chain({\"question\": \"What models use human instructions?\"})\n",
    "\n",
    "chain({\"question\": \"Which are the advantage of each of these models?\"})\n",
    "\n",
    "chain({\"question\": \"Can you elaborate more on point 3?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4b6c6-0e80-46f8-a0fa-f90cdf34cb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain({\"question\":\"Which are the advantage of each of these models?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee460ef-2914-42bf-8aab-dda8955dce1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.qanda_llama2_cont()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ea16b-407a-4068-9ba1-7b194f17fef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "chat_history = \"\"\n",
    "input_list = [{\"user_input\": \"Give me some indications to solve a denial of service attack.\", \"chat_history\":chat_history}]\n",
    "\n",
    "start=time.time()\n",
    "test.llm_chain.generate(input_list)\n",
    "\n",
    "test.llm_chain.generate([{\"user_input\" : \"What question did I asked you previously\"}])\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa58a52-4f28-428f-afbb-72efc29906dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "chat_history = \"\"\n",
    "input_list = [{\"user_input\": \"Give me some indications to solve a denial of service attack.\", \"chat_history\":chat_history}]\n",
    "\n",
    "start=time.time()\n",
    "answer1=test.llm_chain.predict(user_input=\"Give me some indications to solve a denial of service attack.\")\n",
    "print(answer1)\n",
    "answer2=test.llm_chain.predict(user_input=\"What question did I asked you previously\")\n",
    "print(answer2)\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ba867-e64e-4b3c-bd20-5d0ca35f91c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = test.qanda_llama2(\"Can we combine LMMs and OCR?\", with_logging=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bbb003be7165b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T12:28:55.877351200Z",
     "start_time": "2023-12-13T12:28:55.824772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = test.qanda_llama2_withRAG(\"Can we combine LMMs and OCR?\", with_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1b24d-2ff9-42b2-a46b-c0fc385943aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test.qanda_llama2_withRAG(\"Can we combine LLMs and OCR\", with_logging=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c76322-f71b-4272-9207-52c1faf46ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test.ask_question_withRAG(\"Can we combine LLMs and OCR\", with_logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a191cb-186f-4250-95ba-7f956df150ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema.runnable  import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable.utils import ConfigurableField\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4a6e4-f899-45d1-ad8d-d0511e7c9f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=test.embedding_engine)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "llm=test.base_llm\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "########### \n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use 4 sentences maximum and keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),        \n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d6e2b-e96c-4fad-8342-333d9a10d164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_chain_with_source.invoke({question:\"What is Task Decomposition?\"})#\"Always say ```thanks for asking!``` at the end of the answer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f252f-210a-4725-b735-7200f1b9e9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    prompt.invoke(\n",
    "        {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a354bc-c945-4304-a996-ff835b5c04b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "streamer = TextStreamer(test.tokenizer, skip_prompt=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=test.base_llm,\n",
    "    tokenizer=test.tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=0.6,\n",
    "    pad_token_id=test.tokenizer.eos_token_id,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    device=0,\n",
    "    streamer=streamer\n",
    ")\n",
    "pipe(prompts[0])\n",
    "\n",
    "inputs = test.tokenizer(prompts[0], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model_fintuned.generate(**inputs, streamer=streamer, pad_token_id=tokenizer.eos_token_id, max_length=248, temperature=0.8, top_p=0.8,\n",
    "                        repetition_penalty=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cc850-e0ec-4697-b4e9-15e56dea3666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
