{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7eef9-23a4-430a-bc89-d3002293e276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import secrets\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pdb\n",
    "from typing import Any\n",
    "\n",
    "from etl import markdown, pdfs, shared, videos\n",
    "\n",
    "import docstore\n",
    "import vecstore\n",
    "from utils import pretty_log\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import textwrap\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import langchain\n",
    "import time\n",
    "\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "import ciptest_qanda as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b544796-7749-4ab5-888d-476eb2feb652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the embeddings, tokenizer and model\n",
    "embedding_engine = vecstore.get_embedding_engine(allowed_special=\"all\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                          token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             token=True,\n",
    "                                             #  load_in_8bit=True,\n",
    "                                             #  load_in_4bit=True,\n",
    "                               \n",
    "                                             )\n",
    "\n",
    "# Create a streamer and a text generation pipeline\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens=4096,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                streamer=streamer,\n",
    "                )\n",
    "\n",
    "# Create the llm here\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "########### Connecting to the vector storage and load it #############\n",
    "\n",
    "pretty_log(\"connecting to vector storage\")\n",
    "vector_index = vecstore.connect_to_vector_index(vecstore.INDEX_NAME, embedding_engine)\n",
    "pretty_log(\"connected to vector storage\")\n",
    "pretty_log(f\"found {vector_index.index.ntotal} vectors to search over\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e0b55-dedb-4553-b286-3dd1ac943eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## THE LAMA 2 DEMO ############## - MORE GENERIC AND CUSTOMIZED \n",
    "langchain.debug=False \n",
    "\n",
    "# THE OTHER ONE\n",
    "instruction = \"Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input}\"\n",
    "system_prompt = \"\"\"\\\n",
    "\"\"Consider that I'm a beginner in networking and security things. \\n\n",
    "Give me a concise answer with with a single step at a time. \\n\n",
    "Limit your resonse to maximum 128 words.\n",
    "Do not provide any additional text or presentation. Only steps and actions.\n",
    "If possible use concrete names of software or tools that could help on each step.\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "llama_docs_template = \"\"\"\n",
    "[INST]Use the following pieces of context to answer the question. If no context provided, tell the user that you did not find any context about the question and you will answer to the question as you know the best as you can\"\n",
    "{context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llama_docs_template_alternative=\"\"\"\n",
    "[INST]Given the following extracted parts of a long document and a question, create a final answer with \"SOURCES\" that represent exactly the Source name and link given.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "{summaries}\n",
    "\n",
    "FINAL ANSWER: [/INST]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "llama_docs_prompt = PromptTemplate(template=llama_docs_template, input_variables=[\"context\", \"question\"])\n",
    "llama_doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", prompt= llama_docs_prompt, document_variable_name=\"context\", verbose=False)\n",
    "\n",
    "llama_condense_template = \"\"\"\n",
    "[INST]Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question: [/INST]\"\"\"\n",
    "llama_condense_prompt = PromptTemplate(template=llama_condense_template, input_variables=[\"chat_history\", \"question\"])\n",
    "llama_question_generator_chain = LLMChain(llm=llm, prompt=llama_condense_prompt, verbose=False)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llama_v2_chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_index.as_retriever(search_kwargs={'k': 6}),\n",
    "    question_generator=llama_question_generator_chain,\n",
    "    combine_docs_chain=llama_doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# VERY USEFULL FOR checking the sources and context\n",
    "######################################################\n",
    "def test_vectorDatasets_similarityScores_and_responses_no_memory(run_llm_chain: bool):\n",
    "    def sim_que(query : str, run_llm_chain: bool):\n",
    "        pretty_log(\"selecting sources by similarity to query\")\n",
    "        sources_and_scores = vector_index.similarity_search_with_score(query, k=3)\n",
    "    \n",
    "        sources, scores = zip(*sources_and_scores)\n",
    "        print(sources_and_scores)\n",
    "    \n",
    "        if run_llm_chain:\n",
    "            result = llama_doc_chain(\n",
    "                    {\"input_documents\": sources, \"question\": query}, return_only_outputs=True\n",
    "                )\n",
    "        \n",
    "            answer = result[\"output_text\"]\n",
    "            print(answer)\n",
    "    \n",
    "    query1 = \"What models use human instructions?\"\n",
    "    sim_que(query1, run_llm_chain=False)\n",
    "    \n",
    "    query2 = \"Are there any model trained on medical knowledge?\"\n",
    "    sim_que(query2, run_llm_chain=False)\n",
    "    \n",
    "test_vectorDatasets_similarityScores_and_responses_no_memory(run_llm_chain=False)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae01051-b5ea-47b2-a521-291ed6771156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llama_v2_chain({\"question\": \"What models use human instructions?\"}))\n",
    "\n",
    "print(llama_v2_chain({\"question\": \"Which are the advantage of each of these models?\"}))\n",
    "\n",
    "print(llama_v2_chain({\"question\": \"What are the downsides of your last model suggested above ?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test.qanda_llama2_cont()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eee460ef-2914-42bf-8aab-dda8955dce1f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ea16b-407a-4068-9ba1-7b194f17fef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "chat_history = \"\"\n",
    "input_list = [{\"user_input\": \"Give me some indications to solve a denial of service attack.\", \"chat_history\":chat_history}]\n",
    "\n",
    "start=time.time()\n",
    "test.llm_chain.generate(input_list)\n",
    "\n",
    "test.llm_chain.generate([{\"user_input\" : \"What question did I asked you previously\"}])\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa58a52-4f28-428f-afbb-72efc29906dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "chat_history = \"\"\n",
    "input_list = [{\"user_input\": \"Give me some indications to solve a denial of service attack.\", \"chat_history\":chat_history}]\n",
    "\n",
    "start=time.time()\n",
    "answer1=test.llm_chain.predict(user_input=\"Give me some indications to solve a denial of service attack.\")\n",
    "print(answer1)\n",
    "answer2=test.llm_chain.predict(user_input=\"What question did I asked you previously\")\n",
    "print(answer2)\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ba867-e64e-4b3c-bd20-5d0ca35f91c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = test.qanda_llama2(\"Can we combine LMMs and OCR?\", with_logging=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bbb003be7165b",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = test.qanda_llama2_withRAG(\"Can we combine LMMs and OCR?\", with_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1b24d-2ff9-42b2-a46b-c0fc385943aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test.qanda_llama2_withRAG(\"Can we combine LLMs and OCR\", with_logging=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c76322-f71b-4272-9207-52c1faf46ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test.ask_question_withRAG(\"Can we combine LLMs and OCR\", with_logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a191cb-186f-4250-95ba-7f956df150ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema.runnable  import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable.utils import ConfigurableField\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4a6e4-f899-45d1-ad8d-d0511e7c9f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=test.embedding_engine)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "llm=test.base_llm\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "########### \n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use 4 sentences maximum and keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),        \n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d6e2b-e96c-4fad-8342-333d9a10d164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_chain_with_source.invoke({question:\"What is Task Decomposition?\"})#\"Always say ```thanks for asking!``` at the end of the answer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f252f-210a-4725-b735-7200f1b9e9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    prompt.invoke(\n",
    "        {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a354bc-c945-4304-a996-ff835b5c04b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "streamer = TextStreamer(test.tokenizer, skip_prompt=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=test.base_llm,\n",
    "    tokenizer=test.tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=0.6,\n",
    "    pad_token_id=test.tokenizer.eos_token_id,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    device=0,\n",
    "    streamer=streamer\n",
    ")\n",
    "pipe(prompts[0])\n",
    "\n",
    "inputs = test.tokenizer(prompts[0], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model_fintuned.generate(**inputs, streamer=streamer, pad_token_id=tokenizer.eos_token_id, max_length=248, temperature=0.8, top_p=0.8,\n",
    "                        repetition_penalty=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cc850-e0ec-4697-b4e9-15e56dea3666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
